import cv2
import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image
from facenet_pytorch import MTCNN
import torch.nn.functional as F
import numpy as np
import time

# ===============================================================
# [1] ì‚¬ìš©ì ì„¤ì • (ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤!)
# ===============================================================
# 1. ì‚¬ìš©í•  ì˜ìƒ ì†ŒìŠ¤
# - ì›¹ìº  ì‚¬ìš© ì‹œ: 0  (ìˆ«ì 0)
# - íŒŒì¼ ì‚¬ìš© ì‹œ: 'test_video.mp4' (ë¬¸ìì—´)
VIDEO_SOURCE = 0

# 2. í´ë˜ìŠ¤ ì´ë¦„ ( train.py ëŒë¦´ ë•Œ í´ë” ìˆœì„œ(ì•ŒíŒŒë²³ìˆœ)ì™€ ë˜‘ê°™ì•„ì•¼ í•¨!)
# ì˜ˆ: dataset í´ë”ì— jisung, minji, unknownì´ ìˆë‹¤ë©´ -> ['jisung', 'minji', 'unknown']
CLASS_NAMES = ["jisung", "unknown"]

# 3. ë¬¸ ì—´ì–´ì¤„ ì‚¬ëŒ ëª…ë‹¨
AUTHORIZED_USERS = ["jisung"]

# 4. í™•ì‹  ê¸°ì¤€ (ì´ ì ìˆ˜ë³´ë‹¤ ë‚®ìœ¼ë©´ ëª¨ë¥´ëŠ” ì‚¬ëŒ ì·¨ê¸‰)
# 0.7 (70%) ~ 0.8 (80%) ì¶”ì²œ
CONFIDENCE_THRESHOLD = 0.8

# 5. ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
MODEL_PATH = "./model/20251209_052410/face_model.pth"

# ===============================================================


def run_inference():
    print("------------------------------------------------")
    print("ì–¼êµ´ ì¸ì‹ ì‹œìŠ¤í…œ ê°€ë™ (Inference Mode)")
    print(f"íƒ€ê²Ÿ: {CLASS_NAMES}")
    print("------------------------------------------------")

    # 1. ì¥ì¹˜ ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"1. ì‹œìŠ¤í…œ ì¥ì¹˜: {device}")

    # 2. ë°ì´í„° ì „ì²˜ë¦¬
    preprocess = transforms.Compose(
        [
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ]
    )

    # 3. ëª¨ë¸ ë¡œë“œ
    print("2. AI ëª¨ë¸(ResNet18) ë¡œë”© ì¤‘...")
    try:
        # ê»ë°ê¸° ë§Œë“¤ê¸°
        model = models.resnet18(weights=None)
        num_ftrs = model.fc.in_features
        model.fc = nn.Linear(num_ftrs, len(CLASS_NAMES))

        # ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸° (CPU/GPU í˜¸í™˜ì„± ì²˜ë¦¬)
        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
        model = model.to(device)

        # [í•µì‹¬] í‰ê°€ ëª¨ë“œë¡œ ì „í™˜ (Dropout, BatchNorm ê³ ì •)
        model.eval()
        print("   -> ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨! ê²½ë¡œì™€ í´ë˜ìŠ¤ ê°œìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n{e}")
        return

    # 4. ì–¼êµ´ ê°ì§€ê¸° (MTCNN)
    # keep_all=True: í™”ë©´ì— ìˆëŠ” ëª¨ë“  ì‚¬ëŒ ë‹¤ ì°¾ê¸°
    mtcnn = MTCNN(keep_all=True, device=device)

    # 5. ì¹´ë©”ë¼ ì¼œê¸°
    cap = cv2.VideoCapture(VIDEO_SOURCE)
    if not cap.isOpened():
        print("ì¹´ë©”ë¼(ë˜ëŠ” íŒŒì¼)ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("ğŸŸ¢ [Start] í™”ë©´ì— ì–¼êµ´ì„ ë¹„ì¶°ì£¼ì„¸ìš”. (ì¢…ë£Œ: 'q' í‚¤)")

    prev_time = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # FPS ê³„ì‚°
        curr_time = time.time()
        fps = 1 / (curr_time - prev_time)
        prev_time = curr_time

        # OpenCV(BGR) -> PIL(RGB) ë³€í™˜
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(frame_rgb)

        # 1. ì–¼êµ´ ìœ„ì¹˜ ì°¾ê¸° (Detection)
        boxes, _ = mtcnn.detect(pil_img)

        if boxes is not None:
            for box in boxes:
                # ì¢Œí‘œ ì •ìˆ˜ ë³€í™˜ ë° ì˜ˆì™¸ ì²˜ë¦¬
                x1, y1, x2, y2 = [int(b) for b in box]
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)

                # ì–¼êµ´ ì˜ì—­ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ íŒ¨ìŠ¤ (ë…¸ì´ì¦ˆ ë°©ì§€)
                if (x2 - x1) < 20 or (y2 - y1) < 20:
                    continue

                # 2. ì–¼êµ´ ìë¥´ê¸° (Crop)
                face_img = pil_img.crop((x1, y1, x2, y2))

                try:
                    # 3. ì „ì²˜ë¦¬ ë° AI ì˜ˆì¸¡
                    input_tensor = preprocess(face_img).unsqueeze(0).to(device)

                    with torch.no_grad():  # ê³„ì‚° ê¸°ë¡ ë„ê¸° (ì†ë„ í–¥ìƒ)
                        outputs = model(input_tensor)
                        probs = F.softmax(outputs, dim=1)  # í™•ë¥ ë¡œ ë³€í™˜ (0~1)
                        max_prob, idx = torch.max(probs, 1)

                        prob_val = max_prob.item()
                        pred_name = CLASS_NAMES[idx.item()]

                    # 4. ê²°ê³¼ íŒë… (Thresholding)
                    if prob_val < CONFIDENCE_THRESHOLD:
                        # í™•ë¥ ì´ ë‚®ìœ¼ë©´ ëª¨ë¥´ëŠ” ì‚¬ëŒìœ¼ë¡œ ê°„ì£¼
                        final_name = "Unknown"
                        color = (0, 0, 255)  # ë¹¨ê°• (Red)
                        status_text = f"UNKNOWN ({prob_val*100:.1f}%)"
                    else:
                        # í™•ë¥ ì´ ë†’ì„ ë•Œ
                        if pred_name in AUTHORIZED_USERS:
                            final_name = pred_name
                            color = (0, 255, 0)  # ì´ˆë¡ (Green)
                            status_text = (
                                f"OPEN: {pred_name.upper()} ({prob_val*100:.1f}%)"
                            )
                        elif pred_name == "unknown":
                            final_name = "Unknown"
                            color = (0, 0, 255)  # ë¹¨ê°•
                            status_text = f"UNKNOWN ({prob_val*100:.1f}%)"
                        else:
                            final_name = pred_name
                            color = (0, 0, 255)
                            status_text = f"DENIED: {pred_name} ({prob_val*100:.1f}%)"

                    # 5. í™”ë©´ì— ê·¸ë¦¬ê¸°
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)
                    # ê¸€ì ë°°ê²½ ê²€ì€ìƒ‰ ë°•ìŠ¤ (ê°€ë…ì„± UP)
                    cv2.rectangle(
                        frame,
                        (x1, y1 - 35),
                        (x1 + len(status_text) * 18, y1),
                        color,
                        -1,
                    )
                    cv2.putText(
                        frame,
                        status_text,
                        (x1 + 5, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.8,
                        (255, 255, 255),
                        2,
                    )

                except Exception as e:
                    pass  # ì–¼êµ´ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë‚˜ë©´ ë¬´ì‹œí•˜ê³  ë‹¤ìŒ í”„ë ˆì„

        # FPS í‘œì‹œ
        cv2.putText(
            frame,
            f"FPS: {fps:.1f}",
            (10, 30),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.7,
            (0, 255, 255),
            2,
        )

        # í™”ë©´ ì¶œë ¥
        cv2.imshow("AI Face Security System", frame)

        # 'q' ëˆ„ë¥´ë©´ ì¢…ë£Œ
        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

    cap.release()
    cv2.destroyAllWindows()
    print("ğŸ”´ í”„ë¡œê·¸ë¨ ì¢…ë£Œ")


if __name__ == "__main__":
    run_inference()
